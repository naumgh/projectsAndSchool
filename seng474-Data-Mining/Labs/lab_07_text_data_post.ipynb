{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Based Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stopwords are words that are so widely used that they carry very little useful information. \n",
    "\n",
    "Ex: \"a\", \"are\", \"the\", \"is\"\n",
    "\n",
    "Domain specific stop words can also be used.\n",
    "\n",
    "Main reason to exclude these words is to save on memory and proccessing time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stemming is the process of reducing a word to its stem that affixes to suffixes and prefixes or to the roots of words known as \"lemmas\".\n",
    "\n",
    "Ex: \"Dancing\" , \"Danced\", \"Dancer\", \"Dances\" can all be stemmed to the word \"Dance\". "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The basic idea behind TF-IDF is to weight words based on how often they appear in a document, and how rare they are across all documents in a collection.\n",
    "\n",
    "This scheme gives a higher weight to words that are more relevant to the document and less common across all documents, and lower weight to words that are less relevant or more common."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cosine Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cosine similarity between two vectors can range from -1 to 1.\n",
    "\n",
    "A value of 1 indicates that the vectors are identical.\n",
    "\n",
    "A value of 0 indicates that the vectors are orthogonal (unrelated).\n",
    "\n",
    "A value of -1 indicates that the vectors are completely dissimilar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Typically words are assigned a value using a Naive Bayes Classifier.\n",
    "\n",
    "This has a fair ammount of drawbacks, mainly that words are dependent on the context around them, and are not unrelated, but it does a good enough job most of the time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# suppress warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>session</th>\n",
       "      <th>year</th>\n",
       "      <th>country</th>\n",
       "      <th>country_name</th>\n",
       "      <th>speaker</th>\n",
       "      <th>position</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>25</td>\n",
       "      <td>1970</td>\n",
       "      <td>ALB</td>\n",
       "      <td>Albania</td>\n",
       "      <td>Mr. NAS</td>\n",
       "      <td>NaN</td>\n",
       "      <td>33: May I first convey to our President the co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>25</td>\n",
       "      <td>1970</td>\n",
       "      <td>ARG</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Mr. DE PABLO PARDO</td>\n",
       "      <td>NaN</td>\n",
       "      <td>177.\\t : It is a fortunate coincidence that pr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>25</td>\n",
       "      <td>1970</td>\n",
       "      <td>AUS</td>\n",
       "      <td>Australia</td>\n",
       "      <td>Mr. McMAHON</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100.\\t  It is a pleasure for me to extend to y...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>25</td>\n",
       "      <td>1970</td>\n",
       "      <td>AUT</td>\n",
       "      <td>Austria</td>\n",
       "      <td>Mr. KIRCHSCHLAEGER</td>\n",
       "      <td>NaN</td>\n",
       "      <td>155.\\t  May I begin by expressing to Ambassado...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>25</td>\n",
       "      <td>1970</td>\n",
       "      <td>BEL</td>\n",
       "      <td>Belgium</td>\n",
       "      <td>Mr. HARMEL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>176. No doubt each of us, before coming up to ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   session  year country country_name             speaker position  \\\n",
       "0       25  1970     ALB      Albania             Mr. NAS      NaN   \n",
       "1       25  1970     ARG    Argentina  Mr. DE PABLO PARDO      NaN   \n",
       "2       25  1970     AUS    Australia         Mr. McMAHON      NaN   \n",
       "3       25  1970     AUT      Austria  Mr. KIRCHSCHLAEGER      NaN   \n",
       "4       25  1970     BEL      Belgium          Mr. HARMEL      NaN   \n",
       "\n",
       "                                                text  \n",
       "0  33: May I first convey to our President the co...  \n",
       "1  177.\\t : It is a fortunate coincidence that pr...  \n",
       "2  100.\\t  It is a pleasure for me to extend to y...  \n",
       "3  155.\\t  May I begin by expressing to Ambassado...  \n",
       "4  176. No doubt each of us, before coming up to ...  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load some data from UN debates\n",
    "df = pd.read_csv('https://github.com/blueprints-for-text-analytics-python/blueprints-text/raw/master/data/un-general-debates/un-general-debates-blueprint.csv.gz')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(46, 7)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How many speeches from the United Kingdom?\n",
    "df[df['country'] == 'GBR'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(46, 7)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How many speeches from the Canada?\n",
    "df[df['country'] == 'CAN'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\naumh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Download a set of common stop words and then add and remove a few extra for our problem\n",
    "nltk.download('stopwords')\n",
    "\n",
    "stopwords = set(nltk.corpus.stopwords.words('english'))\n",
    "\n",
    "include_stopwords = {'dear', 'regards', 'must', 'would', 'also', \n",
    "                     'canada', 'canadian', 'canadians', 'prime', 'minister', 'province', 'provinces'\n",
    "                     'united', 'kingdom', 'state', 'british', 'irish', 'england','scotland', 'ireland', 'northern'}\n",
    "exclude_stopwords = {'against'}\n",
    "\n",
    "stopwords |= include_stopwords\n",
    "stopwords -= exclude_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         GBR       1.00      1.00      1.00         9\n",
      "         CAN       1.00      1.00      1.00        10\n",
      "\n",
      "    accuracy                           1.00        19\n",
      "   macro avg       1.00      1.00      1.00        19\n",
      "weighted avg       1.00      1.00      1.00        19\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# build a text processing and classifier pipeline\n",
    "# to predict the country (GBR (UK) or Canada) of a speech\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "df2 = df[df['country'].isin(['GBR', 'CAN'])]\n",
    "\n",
    "# Split the dataset into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(df2['text'], df2['country'], test_size=0.2)\n",
    "\n",
    "# Create a pipeline that first transforms the text data into TF-IDF vectors, then applies SVM\n",
    "text_clf = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(stop_words=list(stopwords))),\n",
    "    ('clf', svm.SVC()),\n",
    "])\n",
    "\n",
    "# Train the classifier\n",
    "text_clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict the test set results\n",
    "y_pred = text_clf.predict(X_test)\n",
    "\n",
    "# Print the classification report\n",
    "print(classification_report(y_test, y_pred, target_names=['GBR', 'CAN']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>session</th>\n",
       "      <th>year</th>\n",
       "      <th>country</th>\n",
       "      <th>country_name</th>\n",
       "      <th>speaker</th>\n",
       "      <th>position</th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>25</td>\n",
       "      <td>1970</td>\n",
       "      <td>GBR</td>\n",
       "      <td>United Kingdom</td>\n",
       "      <td>Sir Alec DOUGLASHOME</td>\n",
       "      <td>NaN</td>\n",
       "      <td>110.\\t Mr. President, I should like first to s...</td>\n",
       "      <td>0.049427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>25</td>\n",
       "      <td>1970</td>\n",
       "      <td>CAN</td>\n",
       "      <td>Canada</td>\n",
       "      <td>Mr. SHARP</td>\n",
       "      <td>NaN</td>\n",
       "      <td>\\nThe General Assembly is fortunate indeed to ...</td>\n",
       "      <td>0.107194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>26</td>\n",
       "      <td>1971</td>\n",
       "      <td>GBR</td>\n",
       "      <td>United Kingdom</td>\n",
       "      <td>Sir Alec DOUGLAS-HOME</td>\n",
       "      <td>NaN</td>\n",
       "      <td>79.\\tMr. President, I should like in the begin...</td>\n",
       "      <td>0.110666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204</th>\n",
       "      <td>27</td>\n",
       "      <td>1972</td>\n",
       "      <td>CAN</td>\n",
       "      <td>Canada</td>\n",
       "      <td>Mr. Sharp</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Mr. President, the Canadian delegation looks f...</td>\n",
       "      <td>0.117961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>26</td>\n",
       "      <td>1971</td>\n",
       "      <td>CAN</td>\n",
       "      <td>Canada</td>\n",
       "      <td>Mr- Sharp</td>\n",
       "      <td>NaN</td>\n",
       "      <td>48.\\t  May I first offer you, Sir, the fiM sup...</td>\n",
       "      <td>0.136800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     session  year country    country_name                speaker position  \\\n",
       "18        25  1970     GBR  United Kingdom   Sir Alec DOUGLASHOME      NaN   \n",
       "8         25  1970     CAN          Canada              Mr. SHARP      NaN   \n",
       "105       26  1971     GBR  United Kingdom  Sir Alec DOUGLAS-HOME      NaN   \n",
       "204       27  1972     CAN          Canada             Mr. Sharp       NaN   \n",
       "84        26  1971     CAN          Canada              Mr- Sharp      NaN   \n",
       "\n",
       "                                                  text  sentiment  \n",
       "18   110.\\t Mr. President, I should like first to s...   0.049427  \n",
       "8    \\nThe General Assembly is fortunate indeed to ...   0.107194  \n",
       "105  79.\\tMr. President, I should like in the begin...   0.110666  \n",
       "204  Mr. President, the Canadian delegation looks f...   0.117961  \n",
       "84   48.\\t  May I first offer you, Sir, the fiM sup...   0.136800  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This script creates a new column 'sentiment' in the dataframe, \n",
    "# which contains the sentiment score of the text. \n",
    "# The sentiment score is a float within the range [-1.0, 1.0], \n",
    "# where -1.0 denotes a very negative sentiment, \n",
    "# 1.0 denotes a very positive sentiment, \n",
    "# and values around 0 denote a neutral sentiment.\n",
    "\n",
    "from textblob import TextBlob\n",
    "\n",
    "# Define a function to apply sentiment analysis to a text\n",
    "def get_sentiment(text):\n",
    "    blob = TextBlob(text)\n",
    "    return blob.sentiment.polarity  # returns a value between -1 and 1\n",
    "\n",
    "# Create a new column 'sentiment' in the DataFrame\n",
    "df2['sentiment'] = df2['text'].apply(get_sentiment)\n",
    "\n",
    "# Display the DataFrame\n",
    "df2.head().sort_values(by='sentiment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "country\n",
       "CAN    0.112540\n",
       "GBR    0.107741\n",
       "Name: sentiment, dtype: float64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find average sentiment for each country in df2\n",
    "df2.groupby('country')['sentiment'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "speaker\n",
       "Malcolm Rifkind             0.154614\n",
       "Lawrence Cannon             0.152515\n",
       "Mr. Philip Hammond          0.150041\n",
       "Leonard Edwards             0.145419\n",
       "Mr. MACGUIGAN               0.144525\n",
       "                              ...   \n",
       "Pierre Stewart Pettigrew    0.079461\n",
       "Carrington                  0.078381\n",
       "Paul Martin                 0.075369\n",
       "MacEACHEN                   0.074013\n",
       "Sir Alec DOUGLASHOME        0.049427\n",
       "Name: sentiment, Length: 67, dtype: float64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find average sentiment for each speaker in df2\n",
    "# order the results from most positive to most negative\n",
    "\n",
    "df2.groupby('speaker')['sentiment'].mean().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "year\n",
       "2010    0.139200\n",
       "1989    0.138822\n",
       "2009    0.135793\n",
       "1998    0.135411\n",
       "1990    0.132206\n",
       "1991    0.131901\n",
       "1993    0.131544\n",
       "2015    0.128994\n",
       "1995    0.128064\n",
       "1976    0.127838\n",
       "1994    0.127015\n",
       "2003    0.126351\n",
       "1971    0.123733\n",
       "1996    0.122056\n",
       "1981    0.121182\n",
       "1997    0.119208\n",
       "1988    0.118922\n",
       "1974    0.117654\n",
       "1973    0.115815\n",
       "2007    0.115423\n",
       "2008    0.113624\n",
       "1972    0.112794\n",
       "1984    0.111963\n",
       "1992    0.111177\n",
       "2006    0.108345\n",
       "2014    0.106854\n",
       "2004    0.106391\n",
       "1977    0.104993\n",
       "1978    0.104992\n",
       "2011    0.104768\n",
       "2013    0.103519\n",
       "1983    0.103454\n",
       "1975    0.100152\n",
       "1979    0.099571\n",
       "1985    0.098452\n",
       "2001    0.098188\n",
       "1987    0.095949\n",
       "2000    0.095095\n",
       "1986    0.094259\n",
       "2012    0.093486\n",
       "2005    0.088541\n",
       "1982    0.079657\n",
       "1970    0.078311\n",
       "1980    0.078242\n",
       "1999    0.076149\n",
       "2002    0.060399\n",
       "Name: sentiment, dtype: float64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Can do it by year as well\n",
    "df2.groupby('year')['sentiment'].mean().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STUDENT SECTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        True       0.85      1.00      0.92        11\n",
      "       False       1.00      0.75      0.86         8\n",
      "\n",
      "    accuracy                           0.89        19\n",
      "   macro avg       0.92      0.88      0.89        19\n",
      "weighted avg       0.91      0.89      0.89        19\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>session</th>\n",
       "      <th>year</th>\n",
       "      <th>country</th>\n",
       "      <th>country_name</th>\n",
       "      <th>speaker</th>\n",
       "      <th>position</th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>25</td>\n",
       "      <td>1970</td>\n",
       "      <td>CAN</td>\n",
       "      <td>Canada</td>\n",
       "      <td>Mr. SHARP</td>\n",
       "      <td>NaN</td>\n",
       "      <td>\\nThe General Assembly is fortunate indeed to ...</td>\n",
       "      <td>0.107194</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>25</td>\n",
       "      <td>1970</td>\n",
       "      <td>GBR</td>\n",
       "      <td>United Kingdom</td>\n",
       "      <td>Sir Alec DOUGLASHOME</td>\n",
       "      <td>NaN</td>\n",
       "      <td>110.\\t Mr. President, I should like first to s...</td>\n",
       "      <td>0.049427</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>26</td>\n",
       "      <td>1971</td>\n",
       "      <td>CAN</td>\n",
       "      <td>Canada</td>\n",
       "      <td>Mr- Sharp</td>\n",
       "      <td>NaN</td>\n",
       "      <td>48.\\t  May I first offer you, Sir, the fiM sup...</td>\n",
       "      <td>0.136800</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>26</td>\n",
       "      <td>1971</td>\n",
       "      <td>GBR</td>\n",
       "      <td>United Kingdom</td>\n",
       "      <td>Sir Alec DOUGLAS-HOME</td>\n",
       "      <td>NaN</td>\n",
       "      <td>79.\\tMr. President, I should like in the begin...</td>\n",
       "      <td>0.110666</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204</th>\n",
       "      <td>27</td>\n",
       "      <td>1972</td>\n",
       "      <td>CAN</td>\n",
       "      <td>Canada</td>\n",
       "      <td>Mr. Sharp</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Mr. President, the Canadian delegation looks f...</td>\n",
       "      <td>0.117961</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6988</th>\n",
       "      <td>68</td>\n",
       "      <td>2013</td>\n",
       "      <td>GBR</td>\n",
       "      <td>United Kingdom</td>\n",
       "      <td>Nicholas Clegg</td>\n",
       "      <td>Deputy Prime Minister</td>\n",
       "      <td>In my lifetime, the \\nworld has been sliced up...</td>\n",
       "      <td>0.123690</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7149</th>\n",
       "      <td>69</td>\n",
       "      <td>2014</td>\n",
       "      <td>CAN</td>\n",
       "      <td>Canada</td>\n",
       "      <td>Stephen Harper</td>\n",
       "      <td>Prime Minister</td>\n",
       "      <td>It is both \\nan honour and a pleasure for me t...</td>\n",
       "      <td>0.151988</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7181</th>\n",
       "      <td>69</td>\n",
       "      <td>2014</td>\n",
       "      <td>GBR</td>\n",
       "      <td>United Kingdom</td>\n",
       "      <td>David Cameron</td>\n",
       "      <td>Prime Minister</td>\n",
       "      <td>This year we \\nface extraordinary tests of our...</td>\n",
       "      <td>0.061720</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7343</th>\n",
       "      <td>70</td>\n",
       "      <td>2015</td>\n",
       "      <td>CAN</td>\n",
       "      <td>Canada</td>\n",
       "      <td>Mr. Daniel Jean</td>\n",
       "      <td>Deputy Minister Foreign Affairs</td>\n",
       "      <td>I am honoured to appear before the Assembly to...</td>\n",
       "      <td>0.107947</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7375</th>\n",
       "      <td>70</td>\n",
       "      <td>2015</td>\n",
       "      <td>GBR</td>\n",
       "      <td>United Kingdom</td>\n",
       "      <td>Mr. Philip Hammond</td>\n",
       "      <td>Minister for Foreign Affairs</td>\n",
       "      <td>Nearly 70 years ago, the first meetings of the...</td>\n",
       "      <td>0.150041</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>92 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      session  year country    country_name                speaker  \\\n",
       "8          25  1970     CAN          Canada              Mr. SHARP   \n",
       "18         25  1970     GBR  United Kingdom   Sir Alec DOUGLASHOME   \n",
       "84         26  1971     CAN          Canada              Mr- Sharp   \n",
       "105        26  1971     GBR  United Kingdom  Sir Alec DOUGLAS-HOME   \n",
       "204        27  1972     CAN          Canada             Mr. Sharp    \n",
       "...       ...   ...     ...             ...                    ...   \n",
       "6988       68  2013     GBR  United Kingdom         Nicholas Clegg   \n",
       "7149       69  2014     CAN          Canada         Stephen Harper   \n",
       "7181       69  2014     GBR  United Kingdom          David Cameron   \n",
       "7343       70  2015     CAN          Canada        Mr. Daniel Jean   \n",
       "7375       70  2015     GBR  United Kingdom     Mr. Philip Hammond   \n",
       "\n",
       "                             position  \\\n",
       "8                                 NaN   \n",
       "18                                NaN   \n",
       "84                                NaN   \n",
       "105                               NaN   \n",
       "204                               NaN   \n",
       "...                               ...   \n",
       "6988            Deputy Prime Minister   \n",
       "7149                   Prime Minister   \n",
       "7181                   Prime Minister   \n",
       "7343  Deputy Minister Foreign Affairs   \n",
       "7375     Minister for Foreign Affairs   \n",
       "\n",
       "                                                   text  sentiment  target  \n",
       "8     \\nThe General Assembly is fortunate indeed to ...   0.107194   False  \n",
       "18    110.\\t Mr. President, I should like first to s...   0.049427   False  \n",
       "84    48.\\t  May I first offer you, Sir, the fiM sup...   0.136800   False  \n",
       "105   79.\\tMr. President, I should like in the begin...   0.110666   False  \n",
       "204   Mr. President, the Canadian delegation looks f...   0.117961   False  \n",
       "...                                                 ...        ...     ...  \n",
       "6988  In my lifetime, the \\nworld has been sliced up...   0.123690    True  \n",
       "7149  It is both \\nan honour and a pleasure for me t...   0.151988    True  \n",
       "7181  This year we \\nface extraordinary tests of our...   0.061720    True  \n",
       "7343  I am honoured to appear before the Assembly to...   0.107947    True  \n",
       "7375  Nearly 70 years ago, the first meetings of the...   0.150041    True  \n",
       "\n",
       "[92 rows x 9 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make a copy of df2 called df3 with deep=True \n",
    "df3 = df2.copy(deep=True)\n",
    "\n",
    "\n",
    "# Create a new column called target which is True if the year is greater than or equal to 2000 and False otherwise\n",
    "df3['target']=df3['year'] >= 2000\n",
    "\n",
    "# Split the dataset into training and test sets with a test_size 0.2\n",
    "X_train, X_test, y_train, y_test = train_test_split(df3['text'], df3['target'], test_size=0.2)\n",
    "\n",
    "\n",
    "# Create a pipeline that first transforms the text data into TF-IDF vectors, then applies SVM\n",
    "text_clf = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(stop_words=list(stopwords))),\n",
    "    ('clf', svm.SVC()),\n",
    "])\n",
    "\n",
    "# Train the classifier\n",
    "text_clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict the test set results\n",
    "y_pred = text_clf.predict(X_test)\n",
    "\n",
    "# Print the classification report\n",
    "print(classification_report(y_test, y_pred, target_names=['True', 'False']))\n",
    "df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "year\n",
       "1970    0.078311\n",
       "1971    0.123733\n",
       "1972    0.112794\n",
       "1973    0.115815\n",
       "1974    0.117654\n",
       "1975    0.100152\n",
       "1976    0.127838\n",
       "1977    0.104993\n",
       "1978    0.104992\n",
       "1979    0.099571\n",
       "1980    0.078242\n",
       "1981    0.121182\n",
       "1982    0.079657\n",
       "1983    0.103454\n",
       "1984    0.111963\n",
       "1985    0.098452\n",
       "1986    0.094259\n",
       "1987    0.095949\n",
       "1988    0.118922\n",
       "1989    0.138822\n",
       "1990    0.132206\n",
       "1991    0.131901\n",
       "1992    0.111177\n",
       "1993    0.131544\n",
       "1994    0.127015\n",
       "1995    0.128064\n",
       "1996    0.122056\n",
       "1997    0.119208\n",
       "1998    0.135411\n",
       "1999    0.076149\n",
       "2000    0.095095\n",
       "2001    0.098188\n",
       "2002    0.060399\n",
       "2003    0.126351\n",
       "2004    0.106391\n",
       "2005    0.088541\n",
       "2006    0.108345\n",
       "2007    0.115423\n",
       "2008    0.113624\n",
       "2009    0.135793\n",
       "2010    0.139200\n",
       "2011    0.104768\n",
       "2012    0.093486\n",
       "2013    0.103519\n",
       "2014    0.106854\n",
       "2015    0.128994\n",
       "Name: sentiment, dtype: float64"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find average sentiment for every year in your DataFrame\n",
    "df3.groupby('year')['sentiment'].mean()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
