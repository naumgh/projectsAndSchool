{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression - Gradient Descent\n",
    "--\n",
    "In this part you will build a logistic regression model using Numpy and doing gradient descent. \n",
    "You should complete the following cells (those with comments and no code). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       exam1      exam2  admitted\n",
      "0  34.623660  78.024693         0\n",
      "1  30.286711  43.894998         0\n",
      "2  35.847409  72.902198         0\n",
      "3  60.182599  86.308552         1\n",
      "4  79.032736  75.344376         1\n"
     ]
    }
   ],
   "source": [
    "# read data\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import add_dummy_feature\n",
    "\n",
    "data = pd.read_csv('https://raw.githubusercontent.com/thomouvic/SENG474/main/data/exams_admitted.csv')\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract X and y from data\n",
    "# use features 'exam1' and 'exam2' for X\n",
    "# use feature 'admitted' for y\n",
    "# use pandas.DataFrame.values to convert to numpy arrays\n",
    "X = data[['exam1', 'exam2']].values\n",
    "y = data[['admitted']].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize X\n",
    "# use scikit-learn's built-in function MinMaxScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "X = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add a dummy feature for the intercept\n",
    "# use scikit-learn's built-in function add_dummy_feature\n",
    "from sklearn.preprocessing import add_dummy_feature\n",
    "\n",
    "X = add_dummy_feature(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set m (number of training examples) and n (number of features)\n",
    "# use the shape attribute of X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize theta to zeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define sigmoid function\n",
    "\n",
    "# test your sigmoid function on the value 0, should return 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a hypothesis function called h that takes in: \n",
    "# theta, an instance x, and returns the hypothesis\n",
    "# the hypothesis is the sigmoid of x@theta\n",
    "# use the @ operator for matrix multiplication\n",
    "\n",
    "\n",
    "# test your hypothesis function on the first instance of X, should return [[0.5]]\n",
    "\n",
    "# the above function is vectorized\n",
    "# for example, if instead of a single instance x, we have a matrix X of shape (m,n)\n",
    "# then the hypothesis is a vector of shape (m,1) \n",
    "# where each element is the hypothesis for the corresponding row of X\n",
    "# test it on the first 5 instances of X, should return an array of 0.5's\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a function called J that takes in theta, X, y, and returns the cost\n",
    "# the cost is the average of the log loss over the training examples\n",
    "# the log loss for a single example is -y*log(h(theta,x))-(1-y)*log(1-h(theta,x))\n",
    "# the cost is the average of the log loss over the training examples\n",
    "# use the np.mean function to compute the average\n",
    "# use the np.log function to compute the log\n",
    "# use the @ operator to compute matrix multiplication\n",
    "# use a vectorized implementation, do not use a for loop over the training examples\n",
    "# use the hypothesis function h defined above\n",
    "\n",
    "\n",
    "# test your cost function on the initial all-zero theta, should return 0.6931471805599453\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a function called gradient that takes in theta, X, y, and returns the gradient\n",
    "# the gradient is the average of the gradient over the training examples\n",
    "# use the hypothesis function h defined above\n",
    "# use a vectorized implementation, do not use a for loop over the training examples\n",
    "# use the @ operator to compute matrix multiplication\n",
    "# use the np.mean function to compute the average\n",
    "# use the formula for the gradient given in the lecture\n",
    "# the vectorized formula is X.T@(h(theta,X)-y)/m\n",
    "\n",
    "\n",
    "# test your gradient function on the initial theta \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a function called 'fit' that takes in: \n",
    "# X, y, alpha, num_iters, initial theta, \n",
    "# and returns: final theta, and J_history\n",
    "# inside the function:\n",
    "# initialize theta to the initial theta\n",
    "# initialize J_history to an empty list\n",
    "# for each iteration, using a for loop over num_iters:\n",
    "# update theta by subtracting alpha times the gradient (using the gradient function defined above)\n",
    "# compute the cost J using the cost function defined above and append it to J_history\n",
    "# return final theta, and J_history\n",
    "\n",
    "\n",
    "# create a function called predict that takes in: \n",
    "# theta, and an array X_new of instances, \n",
    "# and returns the predictions\n",
    "# threshold the hypothesis at 0.5\n",
    "# use the hypothesis function h defined above\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# call fit() with the following arguments:\n",
    "# X, y, alpha=0.1, num_iters=10000, initial_theta=np.zeros((n,1))\n",
    "# store the returned values in theta, J_history\n",
    "# uncomment the following line to call fit()\n",
    "\n",
    "# theta, J_history = fit(X, y, alpha=0.1, num_iters=10000, initial_theta=np.zeros((n,1)))\n",
    "\n",
    "# plot the cost over the iterations stored in J_history\n",
    "# you should see the cost decreasing\n",
    "# uncomment the following lines to plot the cost\n",
    "\n",
    "# plt.plot(J_history)\n",
    "# plt.xlabel('Iteration')\n",
    "# plt.ylabel('J')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the data points\n",
    "# use a scatter plot\n",
    "# use the first feature for the x-axis, the second feature for the y-axis\n",
    "# use the actual labels for the color, c=y[:,0]\n",
    "# uncomment the following line to plot the data points\n",
    "# plt.scatter(X[:,1],X[:,2],c=y[:,0])\n",
    "\n",
    "\n",
    "# plot the decision boundary\n",
    "# the decision boundary is the line where the hypothesis is 0.5\n",
    "# the hypothesis is 0.5 when x@theta=0\n",
    "# so the decision boundary is the line where x@theta=0\n",
    "# this is a line in the x1,x2 plane\n",
    "\n",
    "# for plotting the decision boundary, we need two points\n",
    "# create two x1 values, say 0 and 1 (since we scaled to [0,1])\n",
    "# then calculate the corresponding x2 values \n",
    "# using the decision boundary equation\n",
    "# uncomment the following lines to plot the decision boundary\n",
    "# two_x1 = np.array([0, 1])\n",
    "# two_x2 = -(theta[0] + theta[1] * two_x1) / theta[2]\n",
    "\n",
    "# plot the decision boundary as a k-- line. k-- is black dashed line\n",
    "# plt.plot(two_x1, two_x2, \"k--\", linewidth=3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
